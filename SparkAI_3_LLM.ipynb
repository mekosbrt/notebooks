{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mekosbrt/notebooks/blob/master/SparkAI_3_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b52452b1-90e7-4e77-a4aa-4109c5fb79c4",
      "metadata": {
        "language": "python",
        "id": "b52452b1-90e7-4e77-a4aa-4109c5fb79c4"
      },
      "source": [
        "<img src = \"https://github.com/singlestore-labs/spaces-notebooks/blob/e551e274bb67bb1e5081131ee1150cdba713fc43/common/images/singlestore-jupyter.png?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e83a3630-3c94-4bc6-ab80-18563b926388",
      "metadata": {
        "language": "python",
        "id": "e83a3630-3c94-4bc6-ab80-18563b926388"
      },
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(235, 249, 245, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/browser.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Apache Spark + OpenAI for Personalized Banking Services, Part 3</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31d548b8-4622-422a-9bbd-468477ee88fb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:53:55.997919Z",
          "iopub.status.busy": "2024-04-16T15:53:55.997653Z",
          "iopub.status.idle": "2024-04-16T15:53:56.441762Z",
          "shell.execute_reply": "2024-04-16T15:53:56.441128Z",
          "shell.execute_reply.started": "2024-04-16T15:53:55.997898Z"
        },
        "language": "python",
        "trusted": true,
        "id": "31d548b8-4622-422a-9bbd-468477ee88fb"
      },
      "outputs": [],
      "source": [
        "!pip cache purge --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aed2148-2b94-4eb0-95e1-dd6e9a2b3dc9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:53:56.443999Z",
          "iopub.status.busy": "2024-04-16T15:53:56.442934Z",
          "iopub.status.idle": "2024-04-16T15:54:35.202039Z",
          "shell.execute_reply": "2024-04-16T15:54:35.201374Z",
          "shell.execute_reply.started": "2024-04-16T15:53:56.443980Z"
        },
        "language": "python",
        "trusted": true,
        "id": "4aed2148-2b94-4eb0-95e1-dd6e9a2b3dc9",
        "outputId": "ca3284f5-471b-4afa-d91c-83fc12bff595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Collecting package metadata (current_repodata.json): ...working... done\nSolving environment: ...working... done\n\n# All requested packages already installed.\n\n"
        }
      ],
      "source": [
        "!conda install -y --quiet -c conda-forge openjdk pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8b6247-0e7e-4a95-832b-c116cc8192f9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:54:35.203322Z",
          "iopub.status.busy": "2024-04-16T15:54:35.203036Z",
          "iopub.status.idle": "2024-04-16T15:54:40.620410Z",
          "shell.execute_reply": "2024-04-16T15:54:40.619702Z",
          "shell.execute_reply.started": "2024-04-16T15:54:35.203287Z"
        },
        "language": "python",
        "trusted": true,
        "id": "7f8b6247-0e7e-4a95-832b-c116cc8192f9"
      },
      "outputs": [],
      "source": [
        "!pip uninstall langchain-openai -y --quiet\n",
        "!pip install openai==0.28 --quiet\n",
        "!pip install nltk --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3923018-1424-4586-ad14-242ad6eafa78",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:54:40.622642Z",
          "iopub.status.busy": "2024-04-16T15:54:40.622411Z",
          "iopub.status.idle": "2024-04-16T15:54:51.405623Z",
          "shell.execute_reply": "2024-04-16T15:54:51.405116Z",
          "shell.execute_reply.started": "2024-04-16T15:54:40.622619Z"
        },
        "language": "python",
        "trusted": true,
        "id": "c3923018-1424-4586-ad14-242ad6eafa78",
        "outputId": "27bac4d2-0bb9-4d71-93c2-20942aec9f4b"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": "OpenAI API Key: ········\n"
        }
      ],
      "source": [
        "import getpass\n",
        "import openai\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a23d21-013d-412e-a120-4c56d9111af8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:54:51.407043Z",
          "iopub.status.busy": "2024-04-16T15:54:51.406284Z",
          "iopub.status.idle": "2024-04-16T15:54:51.410356Z",
          "shell.execute_reply": "2024-04-16T15:54:51.409879Z",
          "shell.execute_reply.started": "2024-04-16T15:54:51.407020Z"
        },
        "language": "python",
        "trusted": true,
        "id": "a3a23d21-013d-412e-a120-4c56d9111af8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"jars\", exist_ok = True)\n",
        "os.makedirs(\"data\", exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d935b474-40a7-46ec-9ded-51cc2a572ca3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:54:51.411502Z",
          "iopub.status.busy": "2024-04-16T15:54:51.411102Z",
          "iopub.status.idle": "2024-04-16T15:54:51.692509Z",
          "shell.execute_reply": "2024-04-16T15:54:51.691952Z",
          "shell.execute_reply.started": "2024-04-16T15:54:51.411482Z"
        },
        "language": "python",
        "trusted": true,
        "id": "d935b474-40a7-46ec-9ded-51cc2a572ca3",
        "outputId": "47b18eb5-4715-4229-9261-b2e884aec7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "JAR files downloaded successfully\n"
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def download_jar(url, destination):\n",
        "    response = requests.get(url)\n",
        "    with open(destination, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "jar_urls = [\n",
        "    (\"https://repo1.maven.org/maven2/com/singlestore/singlestore-jdbc-client/1.2.1/singlestore-jdbc-client-1.2.1.jar\", \"jars/singlestore-jdbc-client-1.2.1.jar\"),\n",
        "    (\"https://repo1.maven.org/maven2/com/singlestore/singlestore-spark-connector_2.12/4.1.5-spark-3.5.0/singlestore-spark-connector_2.12-4.1.5-spark-3.5.0.jar\", \"jars/singlestore-spark-connector_2.12-4.1.5-spark-3.5.0.jar\"),\n",
        "    (\"https://repo1.maven.org/maven2/org/apache/commons/commons-dbcp2/2.12.0/commons-dbcp2-2.12.0.jar\", \"jars/commons-dbcp2-2.12.0.jar\"),\n",
        "    (\"https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar\", \"jars/commons-pool2-2.12.0.jar\"),\n",
        "    (\"https://repo1.maven.org/maven2/io/spray/spray-json_3/1.3.6/spray-json_3-1.3.6.jar\", \"jars/spray-json_3-1.3.6.jar\")\n",
        "]\n",
        "\n",
        "for url, destination in jar_urls:\n",
        "    download_jar(url, destination)\n",
        "\n",
        "print(\"JAR files downloaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de40d2d2-3558-4d47-bf86-a6d766b8d5d8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:54:51.694548Z",
          "iopub.status.busy": "2024-04-16T15:54:51.694340Z",
          "iopub.status.idle": "2024-04-16T15:54:56.238967Z",
          "shell.execute_reply": "2024-04-16T15:54:56.238280Z",
          "shell.execute_reply.started": "2024-04-16T15:54:51.694534Z"
        },
        "language": "python",
        "trusted": true,
        "id": "de40d2d2-3558-4d47-bf86-a6d766b8d5d8",
        "outputId": "6c2e4944-d614-4c30-be87-9e15cd38410c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "24/04/16 15:54:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/04/16 15:54:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = (SparkSession\n",
        "             .builder\n",
        "             .config(\"spark.jars\", \",\".join([destination for _, destination in jar_urls]))\n",
        "             .appName(\"Spark Webinar\")\n",
        "             .getOrCreate()\n",
        "        )\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a810e8-bc76-4f9e-95b9-51db46304828",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:54:56.239823Z",
          "iopub.status.busy": "2024-04-16T15:54:56.239666Z",
          "iopub.status.idle": "2024-04-16T15:54:56.860164Z",
          "shell.execute_reply": "2024-04-16T15:54:56.859667Z",
          "shell.execute_reply.started": "2024-04-16T15:54:56.239806Z"
        },
        "language": "python",
        "trusted": true,
        "id": "06a810e8-bc76-4f9e-95b9-51db46304828",
        "outputId": "e11833ad-a422-4c61-f11a-743d879bbf80"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw to /home/jovyan/nltk_data...\n[nltk_data]   Package omw is already up-to-date!\n"
        },
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cafc9dbb-0b3f-4054-a208-7d5a1d60e54f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:54:56.861335Z",
          "iopub.status.busy": "2024-04-16T15:54:56.860968Z",
          "iopub.status.idle": "2024-04-16T15:55:04.503301Z",
          "shell.execute_reply": "2024-04-16T15:55:04.502692Z",
          "shell.execute_reply.started": "2024-04-16T15:54:56.861315Z"
        },
        "language": "python",
        "trusted": true,
        "id": "cafc9dbb-0b3f-4054-a208-7d5a1d60e54f"
      },
      "outputs": [],
      "source": [
        "# Define the directory to save the files\n",
        "output_dir = \"data\"\n",
        "\n",
        "# Generate meaningful sentences\n",
        "def generate_meaningful_sentence():\n",
        "    # Choose a random set of synonyms from WordNet\n",
        "    synset = random.choice(list(wn.all_synsets()))\n",
        "\n",
        "    # Generate a sentence\n",
        "    definition = synset.definition()\n",
        "    tokens = word_tokenize(definition)\n",
        "\n",
        "    # Capitalise the first word and end with a period\n",
        "    tokens[0] = tokens[0].capitalize()\n",
        "    tokens[-1] = tokens[-1] + \".\"\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Number of files to generate\n",
        "num_files = 5\n",
        "\n",
        "# Number of sentences in each file\n",
        "num_sentences_per_file = 1\n",
        "\n",
        "# Generate text files\n",
        "for i in range(num_files):\n",
        "    file_path = os.path.join(output_dir, f\"file_{i+1}.txt\")\n",
        "    with open(file_path, \"w\") as file:\n",
        "        for _ in range(num_sentences_per_file):\n",
        "            # Generate a meaningful sentence\n",
        "            sentence = generate_meaningful_sentence()\n",
        "            file.write(sentence + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db8ec8cb-d0e1-4bd9-bec6-fe63c9f09ba3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:55:04.505911Z",
          "iopub.status.busy": "2024-04-16T15:55:04.505402Z",
          "iopub.status.idle": "2024-04-16T15:55:04.529341Z",
          "shell.execute_reply": "2024-04-16T15:55:04.528819Z",
          "shell.execute_reply.started": "2024-04-16T15:55:04.505891Z"
        },
        "language": "python",
        "trusted": true,
        "id": "db8ec8cb-d0e1-4bd9-bec6-fe63c9f09ba3",
        "outputId": "4454651e-eb82-4286-85a8-c13a76f47b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "File: data/file_1.txt\nJump on skis.\n----------------------\nFile: data/file_2.txt\nHybrid willow usually not strongly weeping in habit.\n----------------------\nFile: data/file_3.txt\nWide-ranging light-brown frog of moist North American woodlands especially spruce.\n----------------------\nFile: data/file_4.txt\nA tear gas that is stronger than CN gas but wears off faster ; can be deployed by grenades or cluster bombs ; can cause skin burns and fatal pulmonary edema.\n----------------------\nFile: data/file_5.txt\nIndicating continuing action ; continuously or steadily.\n----------------------\n"
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "for file in data/*.txt; do\n",
        "    echo \"File: $file\"\n",
        "    cat \"$file\"\n",
        "    echo \"----------------------\"\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "159866a7-ddbb-4f71-9486-19e40d407c58",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:55:04.529944Z",
          "iopub.status.busy": "2024-04-16T15:55:04.529810Z",
          "iopub.status.idle": "2024-04-16T15:55:40.694406Z",
          "shell.execute_reply": "2024-04-16T15:55:40.693868Z",
          "shell.execute_reply.started": "2024-04-16T15:55:04.529933Z"
        },
        "language": "python",
        "trusted": true,
        "id": "159866a7-ddbb-4f71-9486-19e40d407c58",
        "outputId": "64709b79-7465-4851-a21f-86643f33a5ba"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": "Password: ········\n"
        }
      ],
      "source": [
        "import getpass\n",
        "\n",
        "host = \"<HOST>\"\n",
        "port = \"3306\"\n",
        "cluster = host + \":\" + port\n",
        "\n",
        "password = getpass.getpass(\"Password:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ab713c-e57a-428b-acff-8167d6a1f575",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:55:40.695617Z",
          "iopub.status.busy": "2024-04-16T15:55:40.695237Z",
          "iopub.status.idle": "2024-04-16T15:55:41.619617Z",
          "shell.execute_reply": "2024-04-16T15:55:41.619041Z",
          "shell.execute_reply.started": "2024-04-16T15:55:40.695597Z"
        },
        "language": "python",
        "trusted": true,
        "id": "25ab713c-e57a-428b-acff-8167d6a1f575"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.datasource.singlestore.ddlEndpoint\", cluster)\n",
        "spark.conf.set(\"spark.datasource.singlestore.user\", \"admin\")\n",
        "spark.conf.set(\"spark.datasource.singlestore.password\", password)\n",
        "spark.conf.set(\"spark.datasource.singlestore.disablePushdown\", \"false\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65b6fc2b-427e-455f-af29-92180c12940e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:56:15.143672Z",
          "iopub.status.busy": "2024-04-16T15:56:15.143467Z",
          "iopub.status.idle": "2024-04-16T15:56:15.610331Z",
          "shell.execute_reply": "2024-04-16T15:56:15.609745Z",
          "shell.execute_reply.started": "2024-04-16T15:56:15.143658Z"
        },
        "language": "sql",
        "trusted": true,
        "id": "65b6fc2b-427e-455f-af29-92180c12940e",
        "outputId": "3be1b874-bb43-4f5a-ceb2-5be8ed9b7155"
      },
      "outputs": [
        {
          "data": {
            "text/html": "<table>\n    <thead>\n        <tr>\n        </tr>\n    </thead>\n    <tbody>\n    </tbody>\n</table>",
            "text/plain": "++\n||\n++\n++"
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "USE spark_demo;\n",
        "\n",
        "DROP TABLE IF EXISTS comments;\n",
        "CREATE TABLE IF NOT EXISTS comments (\n",
        "     value TEXT,\n",
        "     file_name TEXT,\n",
        "     embedding VECTOR(1536) NOT NULL\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aa6262d-7007-44f8-a932-c5deb8d0cfc0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:56:25.915870Z",
          "iopub.status.busy": "2024-04-16T15:56:25.915575Z",
          "iopub.status.idle": "2024-04-16T15:56:25.920937Z",
          "shell.execute_reply": "2024-04-16T15:56:25.920400Z",
          "shell.execute_reply.started": "2024-04-16T15:56:25.915854Z"
        },
        "language": "python",
        "trusted": true,
        "id": "4aa6262d-7007-44f8-a932-c5deb8d0cfc0"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import input_file_name, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Generate embeddings for text\n",
        "def generate_embeddings(text):\n",
        "    # Generate embeddings for text using OpenAI\n",
        "    return openai.Embedding.create(\n",
        "        input = text,\n",
        "        engine = \"text-embedding-3-small\"\n",
        "    ).data[0].embedding\n",
        "\n",
        "# Register the function as a UDF\n",
        "generate_embeddings_udf = udf(generate_embeddings, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b0bdff-b9a1-4992-9470-fbf4a6d7d9f0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:56:33.759811Z",
          "iopub.status.busy": "2024-04-16T15:56:33.759523Z",
          "iopub.status.idle": "2024-04-16T15:56:45.576247Z",
          "shell.execute_reply": "2024-04-16T15:56:45.575687Z",
          "shell.execute_reply.started": "2024-04-16T15:56:33.759794Z"
        },
        "language": "python",
        "trusted": true,
        "id": "b9b0bdff-b9a1-4992-9470-fbf4a6d7d9f0",
        "outputId": "38281d4d-1e05-41df-8fae-3c71212d943b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "                                                                                \r"
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "input_dir = output_dir\n",
        "\n",
        "# Read from the directory\n",
        "df = (spark.readStream\n",
        "    .format(\"text\")\n",
        "    .option(\"path\", input_dir)\n",
        "    .load()\n",
        "    .withColumn(\"file_name\", input_file_name())\n",
        ")\n",
        "\n",
        "# Apply the function to the DataFrame to generate embeddings for each row\n",
        "df_with_embeddings = df.withColumn(\"embedding\", generate_embeddings_udf(\"value\"))\n",
        "\n",
        "# Write each batch of data to SingleStore\n",
        "def write_to_singlestore(df_with_embeddings, epoch_id):\n",
        "    (df_with_embeddings.write\n",
        "         .format(\"singlestore\")\n",
        "         .option(\"loadDataCompression\", \"LZ4\")\n",
        "         .mode(\"append\")\n",
        "         .save(\"spark_demo.comments\")\n",
        "    )\n",
        "\n",
        "# Write the streaming DataFrame to SingleStore using foreachBatch\n",
        "query = (df_with_embeddings.writeStream\n",
        "    .foreachBatch(write_to_singlestore)\n",
        "    .start()\n",
        ")\n",
        "\n",
        "# Wait for the query to finish processing\n",
        "while query.isActive:\n",
        "    time.sleep(1)\n",
        "    if not query.status[\"isDataAvailable\"]:\n",
        "        query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c6b8a6-4164-44ff-9fd4-c12f14ef9219",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:56:56.086316Z",
          "iopub.status.busy": "2024-04-16T15:56:56.086016Z",
          "iopub.status.idle": "2024-04-16T15:56:56.399590Z",
          "shell.execute_reply": "2024-04-16T15:56:56.398995Z",
          "shell.execute_reply.started": "2024-04-16T15:56:56.086298Z"
        },
        "language": "sql",
        "trusted": true,
        "id": "a0c6b8a6-4164-44ff-9fd4-c12f14ef9219",
        "outputId": "734d052f-49bf-4bda-e298-c42a266e3ce1"
      },
      "outputs": [
        {
          "data": {
            "text/html": "<table>\n    <thead>\n        <tr>\n            <th>value</th>\n            <th>file_name</th>\n            <th>embedding</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Indicating continuing action ;</td>\n            <td>file_5.txt</td>\n            <td>[0.0237321071,0.0341514125,0.0182303879,0.03942219</td>\n        </tr>\n        <tr>\n            <td>Wide-ranging light-brown frog </td>\n            <td>file_3.txt</td>\n            <td>[-0.00194031477,-0.0134520773,0.0547336452,0.01292</td>\n        </tr>\n        <tr>\n            <td>Jump on skis.</td>\n            <td>file_1.txt</td>\n            <td>[0.030417813,0.0341594741,0.0108481226,-0.03507469</td>\n        </tr>\n        <tr>\n            <td>A tear gas that is stronger th</td>\n            <td>file_4.txt</td>\n            <td>[0.0302113201,0.0374153629,0.00788259227,0.0550957</td>\n        </tr>\n        <tr>\n            <td>Hybrid willow usually not stro</td>\n            <td>file_2.txt</td>\n            <td>[0.0352415368,0.0366364606,-0.00554679148,0.038584</td>\n        </tr>\n    </tbody>\n</table>",
            "text/plain": "+--------------------------------+------------+----------------------------------------------------+\n|             value              | file_name  |                     embedding                      |\n+--------------------------------+------------+----------------------------------------------------+\n| Indicating continuing action ; | file_5.txt | [0.0237321071,0.0341514125,0.0182303879,0.03942219 |\n| Wide-ranging light-brown frog  | file_3.txt | [-0.00194031477,-0.0134520773,0.0547336452,0.01292 |\n|         Jump on skis.          | file_1.txt | [0.030417813,0.0341594741,0.0108481226,-0.03507469 |\n| A tear gas that is stronger th | file_4.txt | [0.0302113201,0.0374153629,0.00788259227,0.0550957 |\n| Hybrid willow usually not stro | file_2.txt | [0.0352415368,0.0366364606,-0.00554679148,0.038584 |\n+--------------------------------+------------+----------------------------------------------------+"
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "USE spark_demo;\n",
        "\n",
        "SELECT\n",
        "    SUBSTR(value, 1, 30) AS value,\n",
        "    SUBSTR(file_name, LENGTH(file_name) - 9) AS file_name,\n",
        "    SUBSTR(embedding, 1, 50) AS embedding\n",
        "FROM comments;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "240a9c43-6419-4a5a-b5f8-73bc7d5afbca",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:57:20.189000Z",
          "iopub.status.busy": "2024-04-16T15:57:20.188747Z",
          "iopub.status.idle": "2024-04-16T15:57:20.376801Z",
          "shell.execute_reply": "2024-04-16T15:57:20.376280Z",
          "shell.execute_reply.started": "2024-04-16T15:57:20.188984Z"
        },
        "language": "python",
        "trusted": true,
        "id": "240a9c43-6419-4a5a-b5f8-73bc7d5afbca"
      },
      "outputs": [],
      "source": [
        "# Define the text for which you want an embedding\n",
        "text = \"frog\"\n",
        "\n",
        "# Request the embedding\n",
        "response = openai.Embedding.create(\n",
        "    input = text,\n",
        "    engine= \"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# Get the embedding from the response\n",
        "query_string = str(response.data[0].embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67896e42-4413-4816-9b46-51e5a81dfd4a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:57:24.204523Z",
          "iopub.status.busy": "2024-04-16T15:57:24.204274Z",
          "iopub.status.idle": "2024-04-16T15:57:24.208625Z",
          "shell.execute_reply": "2024-04-16T15:57:24.208086Z",
          "shell.execute_reply.started": "2024-04-16T15:57:24.204507Z"
        },
        "language": "python",
        "trusted": true,
        "id": "67896e42-4413-4816-9b46-51e5a81dfd4a"
      },
      "outputs": [],
      "source": [
        "%config SqlMagic.named_parameters = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef4584f1-c945-4c4f-8b62-7dccaa18e49e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:57:28.889668Z",
          "iopub.status.busy": "2024-04-16T15:57:28.889409Z",
          "iopub.status.idle": "2024-04-16T15:57:29.263901Z",
          "shell.execute_reply": "2024-04-16T15:57:29.263381Z",
          "shell.execute_reply.started": "2024-04-16T15:57:28.889652Z"
        },
        "language": "sql",
        "trusted": true,
        "id": "ef4584f1-c945-4c4f-8b62-7dccaa18e49e",
        "outputId": "7c70ef16-8668-4835-fb95-c9cdcf6760b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": "<table>\n    <thead>\n        <tr>\n            <th>value</th>\n            <th>file_name</th>\n            <th>similarity</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>Wide-ranging light-brown frog </td>\n            <td>file_3.txt</td>\n            <td>1.1131780208445137</td>\n        </tr>\n        <tr>\n            <td>Jump on skis.</td>\n            <td>file_1.txt</td>\n            <td>1.2943939467682306</td>\n        </tr>\n        <tr>\n            <td>Indicating continuing action ;</td>\n            <td>file_5.txt</td>\n            <td>1.3149931253318798</td>\n        </tr>\n        <tr>\n            <td>Hybrid willow usually not stro</td>\n            <td>file_2.txt</td>\n            <td>1.319062131851387</td>\n        </tr>\n        <tr>\n            <td>A tear gas that is stronger th</td>\n            <td>file_4.txt</td>\n            <td>1.3730401463567572</td>\n        </tr>\n    </tbody>\n</table>",
            "text/plain": "+--------------------------------+------------+--------------------+\n|             value              | file_name  |     similarity     |\n+--------------------------------+------------+--------------------+\n| Wide-ranging light-brown frog  | file_3.txt | 1.1131780208445137 |\n|         Jump on skis.          | file_1.txt | 1.2943939467682306 |\n| Indicating continuing action ; | file_5.txt | 1.3149931253318798 |\n| Hybrid willow usually not stro | file_2.txt | 1.319062131851387  |\n| A tear gas that is stronger th | file_4.txt | 1.3730401463567572 |\n+--------------------------------+------------+--------------------+"
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "USE spark_demo;\n",
        "\n",
        "SELECT\n",
        "    SUBSTR(value, 1, 30) AS value,\n",
        "    SUBSTR(file_name, LENGTH(file_name) - 9) AS file_name,\n",
        "    embedding <-> :query_string AS similarity\n",
        "FROM comments\n",
        "ORDER BY similarity\n",
        "LIMIT 5;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efad259c-6c50-4cc9-aaaa-1ebd644c75e6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-16T15:57:35.964045Z",
          "iopub.status.busy": "2024-04-16T15:57:35.963799Z",
          "iopub.status.idle": "2024-04-16T15:57:36.941044Z",
          "shell.execute_reply": "2024-04-16T15:57:36.940365Z",
          "shell.execute_reply.started": "2024-04-16T15:57:35.964029Z"
        },
        "language": "python",
        "trusted": true,
        "id": "efad259c-6c50-4cc9-aaaa-1ebd644c75e6"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "jupyterlab": {
      "notebooks": {
        "version_major": 6,
        "version_minor": 4
      }
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "singlestore_cell_default_language": "python",
    "singlestore_connection": {
      "connectionID": "a22b6f8b-b11b-4979-98da-98513e9876e6",
      "defaultDatabase": ""
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}